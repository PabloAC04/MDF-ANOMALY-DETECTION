{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5eaabb0",
   "metadata": {},
   "source": [
    "# üß© Cap√≠tulo 8 ‚Äì Detecci√≥n de Anomal√≠as con Autoencoders\n",
    "\n",
    "En este cap√≠tulo se introduce el enfoque de **Autoencoders**, uno de los primeros modelos de **Deep Learning** aplicados a la detecci√≥n de anomal√≠as.  \n",
    "\n",
    "Su principal fortaleza radica en que **aprenden una representaci√≥n comprimida (codificaci√≥n) de los datos normales y luego intentan reconstruirlos**.  \n",
    "Cuando el modelo encuentra un dato an√≥malo ‚Äîalgo que no encaja en el patr√≥n aprendido‚Äî el **error de reconstrucci√≥n** tiende a ser mayor, lo que permite identificar la anomal√≠a.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ ¬øPor qu√© Autoencoders?\n",
    "\n",
    "La elecci√≥n de Autoencoders responde a varias motivaciones:\n",
    "\n",
    "- Son capaces de **modelar relaciones no lineales complejas** en los datos.  \n",
    "- No requieren etiquetas, solo datos representativos de comportamiento normal.  \n",
    "- Pueden incorporar **estructuras temporales** (mediante secuencias de entrada o variantes recurrentes).  \n",
    "- Funcionan bien como paso previo a modelos m√°s avanzados (ej. LSTM Autoencoders).  \n",
    "- Representan una primera aproximaci√≥n a las capacidades del **Deep Learning** en la detecci√≥n de anomal√≠as.\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ Objetivo de este cap√≠tulo\n",
    "\n",
    "A lo largo del cap√≠tulo se abordar√°n:\n",
    "\n",
    "- Los fundamentos te√≥ricos de los Autoencoders y su uso para detecci√≥n de anomal√≠as.  \n",
    "- La construcci√≥n de un **modelo base** entrenado sobre datos sint√©ticos y MDF.  \n",
    "- El an√°lisis del **error de reconstrucci√≥n** como score de anomal√≠a.  \n",
    "- La comparaci√≥n cualitativa con los modelos cl√°sicos (PCA, Isolation Forest, One-Class SVM).  \n",
    "\n",
    "Adem√°s, se discutir√°n las principales variantes de Autoencoders y se introducir√°n las bases de su extensi√≥n hacia arquitecturas recurrentes como el **LSTM Autoencoder**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Intuici√≥n\n",
    "\n",
    "El Autoencoder es una red neuronal con dos partes:\n",
    "\n",
    "- **Encoder**: comprime los datos de entrada en una representaci√≥n de menor dimensi√≥n (*latent space*).  \n",
    "- **Decoder**: intenta reconstruir la entrada a partir de esa representaci√≥n comprimida.  \n",
    "\n",
    "De forma intuitiva:  \n",
    "\n",
    "> \"Un Autoencoder aprende a copiar los datos normales. Cuando recibe algo raro (an√≥malo), no sabe reconstruirlo bien y el error de reconstrucci√≥n aumenta.\"\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Fundamento Te√≥rico\n",
    "\n",
    "El entrenamiento de un Autoencoder se plantea como la minimizaci√≥n del error de reconstrucci√≥n:\n",
    "\n",
    "$$\n",
    "\\min_{\\theta} \\; L(x, g_{\\theta}(f_{\\theta}(x)))\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $ f_{\\theta}(x) $: encoder (codificaci√≥n al espacio latente).  \n",
    "- $ g_{\\theta}(z) $: decoder (reconstrucci√≥n a partir del latente $z$).  \n",
    "- $ L(\\cdot)$: funci√≥n de p√©rdida, normalmente el **MSE** (Mean Squared Error).  \n",
    "\n",
    "El **score de anomal√≠a** para un punto $x$ se define como:\n",
    "\n",
    "$$\n",
    "s(x) = \\|x - \\hat{x}\\|^2\n",
    "$$\n",
    "\n",
    "Si $s(x)$ supera un umbral, el punto se clasifica como an√≥malo.\n",
    "\n",
    "---\n",
    "\n",
    "## üìê Funci√≥n de Decisi√≥n y Scores\n",
    "\n",
    "- **Salida del modelo**: reconstrucci√≥n $\\hat{x}$ de la entrada $x$.  \n",
    "- **Score de anomal√≠a**: error de reconstrucci√≥n.  \n",
    "  - Error bajo ‚Üí dato normal.  \n",
    "  - Error alto ‚Üí posible anomal√≠a.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Par√°metros Clave\n",
    "\n",
    "- **Dimensi√≥n del espacio latente**: controla la compresi√≥n (similar al n√∫mero de componentes en PCA).  \n",
    "- **N√∫mero de capas y neuronas**: mayor capacidad para aprender relaciones complejas.  \n",
    "- **Funci√≥n de activaci√≥n**: ReLU, tanh, etc.  \n",
    "- **Funci√≥n de p√©rdida**: MSE, MAE o variantes robustas.  \n",
    "- **Regularizaci√≥n**: dropout, weight decay, sparsity para evitar sobreajuste.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Ventajas\n",
    "\n",
    "- Gran capacidad para modelar relaciones no lineales.  \n",
    "- Se adaptan bien a problemas de alta dimensi√≥n.  \n",
    "- Escalables a grandes vol√∫menes de datos.  \n",
    "- Flexibles: pueden extenderse a variantes recurrentes (LSTM, GRU) o convolucionales.\n",
    "\n",
    "---\n",
    "\n",
    "## üö´ Limitaciones\n",
    "\n",
    "- Requieren mayor poder computacional y tiempo de entrenamiento.  \n",
    "- Mayor complejidad en la configuraci√≥n (arquitectura, optimizador, regularizaci√≥n).  \n",
    "- Pueden sobreajustar si se entrenan demasiado tiempo o con datos ruidosos.  \n",
    "- El umbral para anomal√≠as debe definirse cuidadosamente a partir de la distribuci√≥n del error.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Ejemplo concreto\n",
    "\n",
    "Supongamos que queremos monitorizar las **vibraciones de un motor** registradas por tres sensores.  \n",
    "\n",
    "- En condiciones normales, las tres se√±ales est√°n correlacionadas: cuando una aumenta, las otras siguen un patr√≥n consistente.  \n",
    "- Entrenamos un Autoencoder para que aprenda estas relaciones y reconstruya las tres se√±ales a la vez.  \n",
    "\n",
    "Ahora, si en un momento dado el motor empieza a vibrar de forma irregular en uno de los ejes, el Autoencoder **no podr√° reconstruir correctamente esa se√±al**, ya que rompe la correlaci√≥n aprendida.  \n",
    "\n",
    "El error de reconstrucci√≥n en ese punto ser√° mucho mayor que en condiciones normales, permitiendo etiquetarlo como an√≥malo.\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Referencias\n",
    "\n",
    "Hinton, Geoffrey E., and Ruslan R. Salakhutdinov.  \n",
    "**\"Reducing the dimensionality of data with neural networks.\"** Science 313.5786 (2006): 504-507.  \n",
    "\n",
    "Sakurada, Mayu, and Takehisa Yairi.  \n",
    "**\"Anomaly detection using autoencoders with nonlinear dimensionality reduction.\"** Proceedings of the MLSDA 2014 2nd workshop on machine learning for sensory data analysis. 2014.  \n",
    "\n",
    "Zong, Bo, et al.  \n",
    "**\"Deep autoencoding Gaussian mixture model for unsupervised anomaly detection.\"** ICLR 2018.  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
