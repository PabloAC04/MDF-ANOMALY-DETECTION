{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5eaabb0",
   "metadata": {},
   "source": [
    "# Capítulo 8 – Detección de Anomalías con Autoencoders\n",
    "\n",
    "En este capítulo se introduce el enfoque de **Autoencoders**, uno de los primeros modelos de **Deep Learning** aplicados a la detección de anomalías.  \n",
    "\n",
    "Su principal fortaleza radica en que **aprenden una representación comprimida (codificación) de los datos normales y luego intentan reconstruirlos**.  \n",
    "Cuando el modelo encuentra un dato anómalo —algo que no encaja en el patrón aprendido— el **error de reconstrucción** tiende a ser mayor, lo que permite identificar la anomalía.\n",
    "\n",
    "## Motivación\n",
    "\n",
    "La elección de Autoencoders responde a varias razones:\n",
    "\n",
    "- Modelan **relaciones no lineales complejas** en los datos.  \n",
    "- No requieren etiquetas, únicamente muestras representativas del comportamiento normal.  \n",
    "- Permiten incorporar **estructuras temporales** (ventanas deslizantes o variantes recurrentes).  \n",
    "- Funcionan como paso previo a arquitecturas más avanzadas, como los **LSTM Autoencoders**.  \n",
    "- Representan una primera aproximación a las capacidades del **Deep Learning** en el campo de la detección de anomalías.\n",
    "\n",
    "## Objetivo del capítulo\n",
    "\n",
    "Este capítulo abordará:\n",
    "\n",
    "- Los fundamentos teóricos de los Autoencoders y su uso en detección de anomalías.  \n",
    "- La construcción de un modelo base entrenado sobre datos sintéticos y datos reales en formato MDF.  \n",
    "- El análisis del **error de reconstrucción** como score de anomalía.  \n",
    "- La comparación cualitativa con los modelos clásicos (PCA, Isolation Forest, One-Class SVM).  \n",
    "\n",
    "Finalmente, se discutirán las principales variantes de Autoencoders y se introducirá su extensión hacia arquitecturas recurrentes.\n",
    "\n",
    "## Intuición\n",
    "\n",
    "El Autoencoder es una red neuronal formada por dos componentes:\n",
    "\n",
    "- **Encoder**: reduce la dimensionalidad de los datos, generando una representación latente.  \n",
    "- **Decoder**: reconstruye los datos originales a partir de dicha representación.  \n",
    "\n",
    "De forma intuitiva:\n",
    "\n",
    "> Un Autoencoder aprende a copiar los datos normales. Cuando recibe un patrón desconocido o anómalo, no consigue reconstruirlo con precisión y el error de reconstrucción se incrementa.\n",
    "\n",
    "## Fundamento teórico\n",
    "\n",
    "El entrenamiento de un Autoencoder se plantea como la minimización del error de reconstrucción:\n",
    "\n",
    "$$\n",
    "\\min_{\\theta} L(x, g_{\\theta}(f_{\\theta}(x)))\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $f_{\\theta}(x)$: función de codificación (encoder).  \n",
    "- $g_{\\theta}(z)$: función de decodificación (decoder).  \n",
    "- $L(\\cdot)$: función de pérdida, habitualmente el **MSE** (Mean Squared Error).  \n",
    "\n",
    "El score de anomalía para un punto $x$ se define como:\n",
    "\n",
    "$$\n",
    "s(x) = \\|x - \\hat{x}\\|^2\n",
    "$$\n",
    "\n",
    "Si $s(x)$ supera un umbral definido, el punto se clasifica como anómalo.\n",
    "\n",
    "## Función de decisión y scores\n",
    "\n",
    "- **Salida del modelo**: reconstrucción $\\hat{x}$ de la entrada $x$.  \n",
    "- **Score de anomalía**: error de reconstrucción.  \n",
    "  - Valores bajos → dato normal.  \n",
    "  - Valores altos → potencial anomalía.\n",
    "\n",
    "## Parámetros clave\n",
    "\n",
    "- Dimensión del espacio latente.  \n",
    "- Número de capas y neuronas en la red.  \n",
    "- Funciones de activación (ReLU, tanh, etc.).  \n",
    "- Función de pérdida (MSE, MAE o variantes robustas).  \n",
    "- Regularización (dropout, weight decay, sparsity).  \n",
    "\n",
    "## Ventajas\n",
    "\n",
    "- Capturan dependencias no lineales en los datos.  \n",
    "- Escalan adecuadamente a problemas de alta dimensionalidad.  \n",
    "- Pueden adaptarse a grandes volúmenes de datos.  \n",
    "- Flexibles: existen variantes recurrentes y convolucionales.  \n",
    "\n",
    "## Limitaciones\n",
    "\n",
    "- Elevado coste computacional y de entrenamiento en comparación con modelos clásicos.  \n",
    "- Requieren un ajuste cuidadoso de la arquitectura y del proceso de optimización.  \n",
    "- Riesgo de sobreajuste si se entrenan durante demasiado tiempo o con datos ruidosos.  \n",
    "- El umbral de anomalías debe definirse de forma adecuada para cada caso.  \n",
    "\n",
    "## Ejemplo\n",
    "\n",
    "Consideremos la monitorización de las **vibraciones de un motor** a través de tres sensores.  \n",
    "\n",
    "En condiciones normales, las señales están correlacionadas entre sí. Si el motor empieza a vibrar de forma irregular en uno de los ejes, el Autoencoder —entrenado para reconstruir patrones normales— no podrá reproducir esa señal anómala con fidelidad.  \n",
    "\n",
    "El error de reconstrucción crecerá significativamente en ese punto, lo que permitirá marcar la observación como anomalía."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15fd85a",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "- Sakurada, Mayu, and Takehisa Yairi.  \n",
    "  *Anomaly detection using autoencoders with nonlinear dimensionality reduction.* MLSDA, 2014.   \n",
    "\n",
    "- Bui, H., et al.  \n",
    "  *Predicting sector configuration transitions with autoencoder-based anomaly detection.* ICRAT, 2018.\n",
    "\n",
    "- GitHub – thomasdubdub.  \n",
    "  *Autoencoder Anomaly Detection.* Disponible en: [https://github.com/thomasdubdub/autoencoder-anomaly-detection](https://github.com/thomasdubdub/autoencoder-anomaly-detection)  \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
