{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a621430d",
   "metadata": {},
   "source": [
    "# ‚öñÔ∏è Cap√≠tulo 7 ‚Äì Detecci√≥n de Anomal√≠as con One-Class SVM\n",
    "\n",
    "En este cap√≠tulo se presenta el algoritmo **One-Class Support Vector Machine (One-Class SVM)**, una t√©cnica no supervisada para la detecci√≥n de anomal√≠as basada en el marco de las m√°quinas de vectores de soporte.  \n",
    "\n",
    "Su fortaleza principal reside en que **aprende una frontera en el espacio de caracter√≠sticas que encierra la mayor parte de los datos considerados normales**, de modo que todo lo que quede fuera se interpreta como una anomal√≠a.  \n",
    "\n",
    "---\n",
    "\n",
    "## üéØ ¬øPor qu√© One-Class SVM?\n",
    "\n",
    "La elecci√≥n de One-Class SVM responde a varios motivos:\n",
    "\n",
    "- Es un modelo **matem√°ticamente s√≥lido**, derivado de las M√°quinas de Vectores de Soporte.  \n",
    "- Puede modelar **fronteras no lineales complejas** mediante kernels como el RBF.  \n",
    "- Funciona sin necesidad de etiquetas, lo que lo hace adecuado en contextos industriales donde no se dispone de clasificaciones fiables.  \n",
    "- Permite un control expl√≠cito sobre el **porcentaje esperado de anomal√≠as** mediante el hiperpar√°metro $ \\nu $.  \n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ Objetivo de este cap√≠tulo\n",
    "\n",
    "A lo largo del cap√≠tulo se presentar√°n:\n",
    "\n",
    "- Los fundamentos te√≥ricos del One-Class SVM.  \n",
    "- Su aplicaci√≥n sobre datos reales extra√≠dos de archivos MDF.  \n",
    "- El impacto de distintos kernels y par√°metros en la detecci√≥n.  \n",
    "- Una comparaci√≥n cualitativa de sus resultados con los obtenidos por Isolation Forest.  \n",
    "\n",
    "Adem√°s, se discutir√°n las limitaciones pr√°cticas del modelo, en particular su **sensibilidad a la escala de los datos y al ajuste de hiperpar√°metros**.  \n",
    "\n",
    "El cap√≠tulo concluir√° con la transici√≥n hacia modelos de **Deep Learning (Autoencoders)**, que ofrecen mayor capacidad de representaci√≥n.  \n",
    "\n",
    "---\n",
    "\n",
    "## üîç Intuici√≥n\n",
    "\n",
    "El One-Class SVM busca encontrar un **hiperplano en el espacio de caracter√≠sticas** que maximice la separaci√≥n entre los datos normales y el origen.  \n",
    "\n",
    "De forma intuitiva:  \n",
    "\n",
    "> \"One-Class SVM encierra la nube de puntos normales dentro de una frontera flexible. Todo lo que se salga de esa frontera se considera an√≥malo.\"\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Fundamento Te√≥rico\n",
    "\n",
    "El One-Class SVM se formula como un problema de optimizaci√≥n:  \n",
    "\n",
    "Minimizar:  \n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\|w\\|^2 + \\frac{1}{\\nu n}\\sum_{i=1}^n \\xi_i - \\rho\n",
    "$$  \n",
    "\n",
    "Sujeto a:  \n",
    "\n",
    "$$\n",
    "(w \\cdot \\phi(x_i)) \\geq \\rho - \\xi_i, \\quad \\xi_i \\geq 0\n",
    "$$  \n",
    "\n",
    "Donde:\n",
    "\n",
    "- $ \\phi(x) $: transformaci√≥n del espacio de caracter√≠sticas mediante el kernel.  \n",
    "- $ w $: vector de pesos.  \n",
    "- $ \\rho $: umbral que define la frontera.  \n",
    "- $ \\nu \\in (0,1] $: par√°metro que controla la fracci√≥n de outliers y el n√∫mero de vectores de soporte.  \n",
    "\n",
    "La funci√≥n de decisi√≥n resultante es:  \n",
    "\n",
    "$$\n",
    "f(x) = \\text{sign}((w \\cdot \\phi(x)) - \\rho)\n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "## üìê Funci√≥n de Decisi√≥n y Scores\n",
    "\n",
    "- **Salida de predicci√≥n**: $ f(x) \\in \\{+1, -1\\} $  \n",
    "  - +1 ‚Üí Punto normal  \n",
    "  - -1 ‚Üí An√≥malo  \n",
    "\n",
    "- **Score de decisi√≥n**:  \n",
    "  Valores continuos que indican qu√© tan lejos est√° el punto de la frontera.  \n",
    "  - Valores grandes y positivos ‚Üí normales con alta confianza.  \n",
    "  - Valores cercanos a 0 ‚Üí puntos ambiguos.  \n",
    "  - Valores negativos ‚Üí anomal√≠as.  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Par√°metros Clave\n",
    "\n",
    "- **ŒΩ (nu)**: fracci√≥n m√°xima de anomal√≠as esperadas (t√≠picamente entre 0.01 y 0.1).  \n",
    "- **kernel**: define la forma de la frontera (m√°s usado: RBF).  \n",
    "- **Œ≥ (gamma)**: controla la influencia de cada punto en el kernel RBF.  \n",
    "- **coef0**: relevante en kernels polinomiales o sigmoides.  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Ventajas\n",
    "\n",
    "- Basado en una teor√≠a matem√°tica robusta.  \n",
    "- Flexible gracias al uso de kernels.  \n",
    "- Permite ajustar expl√≠citamente el porcentaje esperado de anomal√≠as.  \n",
    "\n",
    "---\n",
    "\n",
    "## üö´ Limitaciones\n",
    "\n",
    "- **Sensibilidad a la escala de los datos** ‚Üí requiere normalizaci√≥n/estandarizaci√≥n previa.  \n",
    "- El rendimiento depende mucho de la elecci√≥n de **ŒΩ** y **Œ≥**.  \n",
    "- Computacionalmente m√°s costoso que Isolation Forest en datasets grandes.  \n",
    "- Puede sufrir de **overfitting** si se ajustan mal los par√°metros del kernel.  \n",
    "\n",
    "---\n",
    "\n",
    "### üîß Ejemplo concreto\n",
    "\n",
    "Supongamos que se analiza el **consumo el√©ctrico de un veh√≠culo h√≠brido** en funci√≥n de dos variables:\n",
    "\n",
    "- **Nivel de carga de la bater√≠a (%)**  \n",
    "- **Velocidad del veh√≠culo (km/h)**  \n",
    "\n",
    "En condiciones normales, el sistema de gesti√≥n de energ√≠a mantiene una relaci√≥n **no lineal**:  \n",
    "- A velocidades bajas (20‚Äì50 km/h) el coche utiliza m√°s bater√≠a.  \n",
    "- A velocidades medias (80‚Äì120 km/h) predomina el motor de combusti√≥n y el consumo el√©ctrico desciende.  \n",
    "- En frenadas o descensos, incluso puede observarse **recuperaci√≥n de energ√≠a** (consumo negativo).\n",
    "\n",
    "El One-Class SVM entrenado sobre datos de conducci√≥n normal aprender√° una **frontera curva y cerrada** que encierra esta nube de puntos con forma caracter√≠stica.  \n",
    "\n",
    "Ahora, imaginemos que aparece un punto con:  \n",
    "\n",
    "- Velocidad = 100 km/h  \n",
    "- Nivel de bater√≠a cayendo bruscamente (como si se descargara a alta potencia)  \n",
    "\n",
    "Esta situaci√≥n es **inconsistente con el patr√≥n aprendido**: a esa velocidad, la bater√≠a deber√≠a estar estable o descargarse muy lentamente.  \n",
    "\n",
    "‚û°Ô∏è El modelo lo marcar√° como **an√≥malo**, ya que cae fuera de la regi√≥n habitual, revelando un posible fallo en el sistema de gesti√≥n energ√©tica o en la bater√≠a.\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Referencias\n",
    "\n",
    "Sch√∂lkopf, Bernhard, et al.  \n",
    "**\"Estimating the support of a high-dimensional distribution.\"** Neural computation 13.7 (2001): 1443-1471.  \n",
    "\n",
    "Tax, David MJ, and Robert PW Duin.  \n",
    "**\"Support vector data description.\"** Machine learning 54 (2004): 45-66.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9841f9b6",
   "metadata": {},
   "source": [
    "## üß™ Validaci√≥n experimental de One-Class SVM\n",
    "\n",
    "Para evaluar el rendimiento de nuestro **One-Class SVM**, seguimos el mismo protocolo definido en el resto de modelos:\n",
    "\n",
    "1. **Partici√≥n temporal**: generamos una serie sint√©tica con anomal√≠as inyectadas y dividimos en *train*, *val* y *test*. Unimos **train+val** para la b√∫squeda de hiperpar√°metros y reservamos **test** para la evaluaci√≥n final.\n",
    "2. **Validaci√≥n con TSCV**: utilizamos un **Time Series Cross-Validation** con ventanas deslizantes (par√°metros `P_train`, `num_windows`, etc.) para respetar la causalidad temporal y evitar *look-ahead*.\n",
    "3. **Preprocesado consistente**: aplicamos el mismo pipeline de preprocesado (detecci√≥n/mitigaci√≥n de outliers con filtro de Hampel, tratamiento estacional si procede y normalizaci√≥n). Este paso es especialmente cr√≠tico en SVM, ya que el algoritmo es muy sensible a la escala de las variables.\n",
    "4. **B√∫squeda de hiperpar√°metros**: realizamos un *grid search* sobre:\n",
    "   - `ŒΩ` (*nu*): fracci√≥n m√°xima de anomal√≠as esperadas (controla el margen de decisi√≥n).\n",
    "   - `kernel`: tipo de kernel (RBF, lineal, polin√≥mico).\n",
    "   - `Œ≥` (*gamma*): par√°metro del kernel RBF que define la influencia local de cada punto.\n",
    "5. **M√©tricas**: calculamos **precision**, **recall**, **F1**, **ROC-AUC**, **NAB** y **window coverage**. Seleccionamos las 5 mejores configuraciones de validaci√≥n seg√∫n **NAB**.\n",
    "6. **Evaluaci√≥n final**: reentrenamos las configuraciones *top-5* en **train+val** y medimos en **test**. Para cada configuraci√≥n mostramos la curva ROC y la visualizaci√≥n de anomal√≠as detectadas frente a anomal√≠as reales en las se√±ales.\n",
    "\n",
    "> Nota: en esta implementaci√≥n, los par√°metros `ŒΩ` y `Œ≥` juegan un papel an√°logo al `contamination` en Isolation Forest:  \n",
    "> - **ŒΩ alto** ‚Üí el modelo considera m√°s puntos como an√≥malos (mayor *recall*, menor *precision*).  \n",
    "> - **ŒΩ bajo** ‚Üí el modelo es m√°s estricto, detecta menos anomal√≠as pero con mayor confianza.  \n",
    "> El *grid search* permite equilibrar esta sensibilidad de acuerdo con las m√©tricas objetivo.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
