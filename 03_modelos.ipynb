{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3357123b",
   "metadata": {},
   "source": [
    "# ü§ñ Cap√≠tulo 3 - Modelos de Detecci√≥n de Anomal√≠as en Series Temporales\n",
    "\n",
    "La detecci√≥n de anomal√≠as en series temporales puede abordarse desde distintos enfoques metodol√≥gicos. En la literatura suelen diferenciarse tres grandes familias: **m√©todos estad√≠sticos tradicionales**, **t√©cnicas de machine learning cl√°sico** y **modelos basados en deep learning**.  \n",
    "\n",
    "Cada una de estas aproximaciones refleja una forma distinta de entender qu√© significa \"an√≥malo\" y qu√© herramientas usar para identificarlo. A continuaci√≥n se presentan sus caracter√≠sticas principales.\n",
    "\n",
    "---\n",
    "\n",
    "## üìà 3.1 M√©todos estad√≠sticos tradicionales\n",
    "\n",
    "Los enfoques estad√≠sticos son los m√°s antiguos y parten de la idea de que una anomal√≠a es cualquier observaci√≥n que **se desv√≠a significativamente del comportamiento esperado seg√∫n un modelo probabil√≠stico**.  \n",
    "\n",
    "- **C√≥mo funcionan**:  \n",
    "  Se modela la serie temporal bajo ciertos supuestos (linealidad, estacionariedad, distribuci√≥n normal de errores). A partir de ese modelo se calculan **intervalos de confianza** o **umbrales de control**; si una observaci√≥n queda fuera de esos l√≠mites, se marca como anomal√≠a.  \n",
    "\n",
    "- **Escenarios t√≠picos de uso**:  \n",
    "  - Procesos industriales donde los sensores siguen distribuciones conocidas.  \n",
    "  - Se√±ales univariantes con ruido bajo y comportamiento regular.  \n",
    "  - Series estacionarias o con din√°mica bien capturada por modelos ARIMA.  \n",
    "\n",
    "- **Ventajas**:  \n",
    "  - Gran interpretabilidad: es f√°cil explicar por qu√© un punto fue marcado como an√≥malo.  \n",
    "  - Bajo coste computacional, adecuado para monitorizaci√≥n en tiempo real.  \n",
    "  - Adecuado cuando los datos son escasos pero cumplen los supuestos.  \n",
    "\n",
    "- **Limitaciones**:  \n",
    "  - Poca capacidad para detectar anomal√≠as en datos **no estacionarios**, multivariantes o con fuertes no linealidades.  \n",
    "  - No escalan bien a dominios con muchas variables correlacionadas.  \n",
    "  - Los supuestos probabil√≠sticos suelen ser demasiado restrictivos para datos complejos.  \n",
    "\n",
    "---\n",
    "\n",
    "## ü§ù 3.2 M√©todos de Machine Learning cl√°sico\n",
    "\n",
    "El aprendizaje autom√°tico cl√°sico introduce algoritmos m√°s flexibles que ya **no dependen de supuestos estad√≠sticos r√≠gidos**. Aqu√≠, las anomal√≠as se entienden como **observaciones poco frecuentes o dif√≠ciles de agrupar en patrones comunes**.  \n",
    "\n",
    "- **C√≥mo funcionan**:  \n",
    "  La mayor√≠a de t√©cnicas se basan en **aprendizaje no supervisado o semi-supervisado**:  \n",
    "  - Modelos como Isolation Forest o One-Class SVM aprenden una frontera que engloba la \"normalidad\".  \n",
    "  - Algoritmos de clustering (k-means, DBSCAN) identifican grupos de comportamiento frecuente; los puntos alejados de esos grupos se marcan como an√≥malos.  \n",
    "\n",
    "- **Escenarios t√≠picos de uso**:  \n",
    "  - Series multivariantes donde las dependencias entre variables aportan m√°s informaci√≥n que cada se√±al por separado.  \n",
    "  - Casos con muchas observaciones y pocos supuestos sobre la distribuci√≥n de los datos.  \n",
    "  - Detecci√≥n temprana de fallos en sistemas donde no siempre se conocen las reglas f√≠sicas exactas.  \n",
    "\n",
    "- **Ventajas**:  \n",
    "  - Mayor flexibilidad que los modelos estad√≠sticos: funcionan en dominios no lineales.  \n",
    "  - Se adaptan a distintos vol√∫menes de datos y a diferentes dimensiones.  \n",
    "  - No requieren etiquetas completas (importante en anomal√≠as, donde los datos etiquetados son escasos).  \n",
    "\n",
    "- **Limitaciones**:  \n",
    "  - Dependencia de la **elecci√≥n de hiperpar√°metros** (ej. n√∫mero de clusters, profundidad de √°rboles).  \n",
    "  - No siempre modelan expl√≠citamente la estructura temporal: muchos algoritmos tratan los datos como \"puntos independientes\".  \n",
    "  - Interpretabilidad reducida respecto a m√©todos estad√≠sticos.  \n",
    "\n",
    "---\n",
    "\n",
    "## üß† 3.3 M√©todos basados en Deep Learning\n",
    "\n",
    "Con el auge del deep learning han surgido modelos capaces de capturar **dependencias temporales complejas y no lineales**. Aqu√≠ las anomal√≠as se interpretan como **patrones que no pueden ser reconstruidos o predichos adecuadamente por una red neuronal entrenada sobre datos normales**.  \n",
    "\n",
    "- **C√≥mo funcionan**:  \n",
    "  - **Redes recurrentes (LSTM, GRU)**: aprenden la din√°mica temporal de la serie y marcan como an√≥malos los puntos con error de predicci√≥n elevado.  \n",
    "  - **Autoencoders**: aprenden a reconstruir se√±ales normales; si una secuencia no se reconstruye bien, se considera an√≥mala.  \n",
    "  - **Transformers y modelos de atenci√≥n**: capturan dependencias de largo alcance, muy √∫tiles en series largas y multivariantes.  \n",
    "\n",
    "- **Escenarios t√≠picos de uso**:  \n",
    "  - Series de alta dimensionalidad y con relaciones complejas entre variables.  \n",
    "  - Datos abundantes, donde el entrenamiento intensivo est√° justificado.  \n",
    "  - Casos donde la detecci√≥n temprana es cr√≠tica y se necesita capturar patrones sutiles.  \n",
    "\n",
    "- **Ventajas**:  \n",
    "  - Gran capacidad de representaci√≥n: detectan patrones dif√≠ciles de capturar con otros m√©todos.  \n",
    "  - Escalan a datos multivariantes y dependencias de largo alcance.  \n",
    "  - Han demostrado rendimiento superior en benchmarks recientes de detecci√≥n de anomal√≠as.  \n",
    "\n",
    "- **Limitaciones**:  \n",
    "  - Elevado coste computacional y necesidad de hardware especializado.  \n",
    "  - Mayor riesgo de sobreajuste, especialmente con datasets peque√±os.  \n",
    "  - Menor interpretabilidad: los modelos son m√°s dif√≠ciles de explicar a usuarios finales.  \n",
    "\n",
    "---\n",
    "\n",
    "## üìå Conclusi√≥n de la secci√≥n\n",
    "\n",
    "Los tres enfoques ofrecen perspectivas complementarias:  \n",
    "- Los **m√©todos estad√≠sticos** son simples y explicativos, adecuados para entornos con supuestos claros.  \n",
    "- El **machine learning cl√°sico** aporta flexibilidad y un balance entre interpretabilidad y capacidad de generalizaci√≥n.  \n",
    "- El **deep learning** ofrece la potencia necesaria para dominios complejos, a costa de mayor coste computacional y menor transparencia.  \n",
    "\n",
    "En el caso en el que nos vamos a mover nosotros, tenemos datos **multivariantes**, de los cuales **no existen supuestos claros** sobre su distribuci√≥n ni suelen presentar tendencias o estacionalidad marcada. Esto hace que los m√©todos estad√≠sticos no sean, en principio, la elecci√≥n m√°s adecuada. Sin embargo, para fines comparativos se implementar√° **al menos un modelo estad√≠stico**, junto con **varios modelos de machine learning cl√°sico** y **otros de deep learning**. El objetivo ser√° evaluar el **comportamiento fundamental** de cada enfoque, manteniendo la simplicidad en su implementaci√≥n, sin a√±adir t√©cnicas accesorias que puedan enmascarar sus caracter√≠sticas b√°sicas.\n",
    "\n",
    "En las siguientes secciones se analizar√°n en detalle algunos modelos concretos de estas familias que ser√°n aplicados a los datasets introducidos en el cap√≠tulo anterior, con el fin de seleccionar el que mejor se comporta con los diferentes datos.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
