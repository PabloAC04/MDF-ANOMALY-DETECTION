{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3357123b",
   "metadata": {},
   "source": [
    "# Capítulo 3 - Modelos de Detección de Anomalías en Series Temporales\n",
    "\n",
    "La detección de anomalías en series temporales puede abordarse desde distintos enfoques metodológicos. En la literatura suelen diferenciarse tres grandes familias: **métodos estadísticos tradicionales**, **técnicas de machine learning clásico** y **modelos basados en deep learning**.  \n",
    "\n",
    "Cada una de estas aproximaciones refleja una forma distinta de entender qué significa \"anómalo\" y qué herramientas usar para identificarlo. A continuación se presentan sus características principales.\n",
    "\n",
    "## 3.1 Métodos estadísticos tradicionales\n",
    "\n",
    "Los enfoques estadísticos son los más antiguos y parten de la idea de que una anomalía es cualquier observación que **se desvía significativamente del comportamiento esperado según un modelo probabilístico**.  \n",
    "\n",
    "- **Cómo funcionan**:  \n",
    "  Se modela la serie temporal bajo ciertos supuestos (linealidad, estacionariedad, distribución normal de errores). A partir de ese modelo se calculan **intervalos de confianza** o **umbrales de control**; si una observación queda fuera de esos límites, se marca como anomalía.  \n",
    "\n",
    "- **Escenarios típicos de uso**:  \n",
    "  - Procesos industriales donde los sensores siguen distribuciones conocidas.  \n",
    "  - Señales univariantes con ruido bajo y comportamiento regular.  \n",
    "  - Series estacionarias o con dinámica bien capturada por modelos ARIMA.  \n",
    "\n",
    "- **Ventajas**:  \n",
    "  - Gran interpretabilidad: es fácil explicar por qué un punto fue marcado como anómalo.  \n",
    "  - Bajo coste computacional, adecuado para monitorización en tiempo real.  \n",
    "  - Adecuado cuando los datos son escasos pero cumplen los supuestos.  \n",
    "\n",
    "- **Limitaciones**:  \n",
    "  - Poca capacidad para detectar anomalías en datos **no estacionarios**, multivariantes o con fuertes no linealidades.  \n",
    "  - No escalan bien a dominios con muchas variables correlacionadas.  \n",
    "  - Los supuestos probabilísticos suelen ser demasiado restrictivos para datos complejos.  \n",
    "\n",
    "## 3.2 Métodos de Machine Learning clásico\n",
    "\n",
    "El aprendizaje automático clásico introduce algoritmos más flexibles que ya **no dependen de supuestos estadísticos rígidos**. Aquí, las anomalías se entienden como **observaciones poco frecuentes o difíciles de agrupar en patrones comunes**.  \n",
    "\n",
    "- **Cómo funcionan**:  \n",
    "  La mayoría de técnicas se basan en **aprendizaje no supervisado o semi-supervisado**:  \n",
    "  - Modelos como Isolation Forest o One-Class SVM aprenden una frontera que engloba la \"normalidad\".  \n",
    "  - Algoritmos de clustering (k-means, DBSCAN) identifican grupos de comportamiento frecuente; los puntos alejados de esos grupos se marcan como anómalos.  \n",
    "\n",
    "- **Escenarios típicos de uso**:  \n",
    "  - Series multivariantes donde las dependencias entre variables aportan más información que cada señal por separado.  \n",
    "  - Casos con muchas observaciones y pocos supuestos sobre la distribución de los datos.  \n",
    "  - Detección temprana de fallos en sistemas donde no siempre se conocen las reglas físicas exactas.  \n",
    "\n",
    "- **Ventajas**:  \n",
    "  - Mayor flexibilidad que los modelos estadísticos: funcionan en dominios no lineales.  \n",
    "  - Se adaptan a distintos volúmenes de datos y a diferentes dimensiones.  \n",
    "  - No requieren etiquetas completas (importante en anomalías, donde los datos etiquetados son escasos).  \n",
    "\n",
    "- **Limitaciones**:  \n",
    "  - Dependencia de la **elección de hiperparámetros** (ej. número de clusters, profundidad de árboles).  \n",
    "  - No siempre modelan explícitamente la estructura temporal: muchos algoritmos tratan los datos como \"puntos independientes\".  \n",
    "  - Interpretabilidad reducida respecto a métodos estadísticos.  \n",
    "\n",
    "## 3.3 Métodos basados en Deep Learning\n",
    "\n",
    "Con el auge del deep learning han surgido modelos capaces de capturar **dependencias temporales complejas y no lineales**. Aquí las anomalías se interpretan como **patrones que no pueden ser reconstruidos o predichos adecuadamente por una red neuronal entrenada sobre datos normales**.  \n",
    "\n",
    "- **Cómo funcionan**:  \n",
    "  - **Redes recurrentes (LSTM, GRU)**: aprenden la dinámica temporal de la serie y marcan como anómalos los puntos con error de predicción elevado.  \n",
    "  - **Autoencoders**: aprenden a reconstruir señales normales; si una secuencia no se reconstruye bien, se considera anómala.  \n",
    "  - **Transformers y modelos de atención**: capturan dependencias de largo alcance, muy útiles en series largas y multivariantes.  \n",
    "\n",
    "- **Escenarios típicos de uso**:  \n",
    "  - Series de alta dimensionalidad y con relaciones complejas entre variables.  \n",
    "  - Datos abundantes, donde el entrenamiento intensivo está justificado.  \n",
    "  - Casos donde la detección temprana es crítica y se necesita capturar patrones sutiles.  \n",
    "\n",
    "- **Ventajas**:  \n",
    "  - Gran capacidad de representación: detectan patrones difíciles de capturar con otros métodos.  \n",
    "  - Escalan a datos multivariantes y dependencias de largo alcance.  \n",
    "  - Han demostrado rendimiento superior en benchmarks recientes de detección de anomalías.  \n",
    "\n",
    "- **Limitaciones**:  \n",
    "  - Elevado coste computacional y necesidad de hardware especializado.  \n",
    "  - Mayor riesgo de sobreajuste, especialmente con datasets pequeños.  \n",
    "  - Menor interpretabilidad: los modelos son más difíciles de explicar a usuarios finales.  \n",
    "\n",
    "## Conclusión de la sección\n",
    "\n",
    "Los tres enfoques ofrecen perspectivas complementarias:  \n",
    "- Los **métodos estadísticos** son simples y explicativos, adecuados para entornos con supuestos claros.  \n",
    "- El **machine learning clásico** aporta flexibilidad y un balance entre interpretabilidad y capacidad de generalización.  \n",
    "- El **deep learning** ofrece la potencia necesaria para dominios complejos, a costa de mayor coste computacional y menor transparencia.  \n",
    "\n",
    "En el caso en el que nos vamos a mover nosotros, tenemos datos **multivariantes**, de los cuales **no existen supuestos claros** sobre su distribución ni suelen presentar tendencias o estacionalidad marcada. Esto hace que los métodos estadísticos no sean, en principio, la elección más adecuada. Sin embargo, para fines comparativos se implementará **al menos un modelo estadístico**, junto con **varios modelos de machine learning clásico** y **otros de deep learning**. El objetivo será evaluar el **comportamiento fundamental** de cada enfoque, manteniendo la simplicidad en su implementación, sin añadir técnicas accesorias que puedan enmascarar sus características básicas.\n",
    "\n",
    "En las siguientes secciones se analizarán en detalle algunos modelos concretos de estas familias que serán aplicados a los datasets introducidos en el capítulo anterior, con el fin de seleccionar el que mejor se comporta con los diferentes datos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
